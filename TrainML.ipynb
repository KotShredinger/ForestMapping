{"cells":[{"cell_type":"code","source":["!pip install geojson\n","!pip install rasterio"],"metadata":{"id":"Xy1WsWBoQ74s"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GelFxdkvX5nf"},"outputs":[],"source":["import os\n","import gdal\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import cv2\n","import pandas as pd\n","import rasterio\n","from rasterio.mask import mask\n","import geojson\n","import random\n","import argparse\n","from osgeo import gdal\n","from osgeo.gdalconst import GDT_Float32\n","import sys\n","\n","from sklearn.base import clone\n","from sklearn import metrics\n","from sklearn.linear_model import RidgeClassifier\n","from sklearn.ensemble import ExtraTreesClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n","from xgboost import XGBClassifier\n","from sklearn.svm import SVC\n","from sklearn.gaussian_process import GaussianProcessClassifier\n","from sklearn.multiclass import OneVsOneClassifier\n","from sklearn.multiclass import OneVsRestClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ycxPcmWngal9"},"outputs":[],"source":["def get_img(snapshot):\n","    plt.figure(figsize=(7, 7))\n","    plt.axis(\"off\")\n","    plt.imshow(cv2.merge([snapshot[3], snapshot[2], snapshot[1]]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jPQDkHXuWvwe"},"outputs":[],"source":["def create_df(field_g, field_class, max_canal, num_def_field):\n","    df_def_g = pd.DataFrame(columns=[i for i in range(max_canal)]+['class', 'id'])\n","    for field_i in field_g:\n","        df_def_i = pd.DataFrame(field_i.reshape(max_canal, -1).transpose())\n","        df_def_i['id'] = num_def_field\n","        num_def_field += 1\n","        #df_def_i = df_def_i.loc[(df_def_i[2]>0)&(df_def_i[3]>0)]\n","        df_def_g = pd.concat([df_def_g, df_def_i], ignore_index=True)\n","    df_def_g['class'] = field_class\n","    return df_def_g, num_def_field"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"881nkoAZcvz_"},"outputs":[],"source":["def get_id_test(df_def, part_test, max_shuffle=30):\n","    df_def_count = df_def.groupby('id', as_index=False).count()[['id', 'class']]\n","    max_count = df_def_count['class'].sum()\n","    min_count = 1\n","    min_id_test = []\n","    i_def = 0\n","    while i_def<max_shuffle and abs(min_count-part_test)>0.01:\n","        sum_count = 0\n","        id_test = []\n","        for index_row, df_count_row in df_def_count.sample(frac=1).iterrows():\n","            sum_count += df_count_row['class']\n","            id_test.append(df_count_row['id'])\n","            cur_count = sum_count/max_count\n","            if cur_count>=part_test-0.02:\n","                if abs(cur_count-part_test)<abs(min_count-part_test):\n","                    min_count = cur_count\n","                    min_id_test = id_test\n","                i_def += 1\n","                break\n","    if min_count>part_test*2 or part_test-0.96>0:\n","        min_id_test = []\n","    return min_id_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ANbe4tjA0rE"},"outputs":[],"source":["def create_prepared_df(list_df_def, list_data_def, remove_cloud=True, with_class=False):\n","    df_def = pd.DataFrame()\n","    for data_def, df_i_def in zip(list_data_def, list_df_def):\n","        df_i_def = df_i_def.rename(columns={x:str(x)+'_'+data_def for x in (df_i_def.columns[:-2] if with_class else df_i_def.columns)})\n","        df_def = df_def.join(df_i_def[(df_i_def.columns[:-2] if with_class else df_i_def.columns)], how=\"outer\")\n","    if with_class:\n","        df_def['class'] = df_i_def['class']\n","    if remove_cloud:\n","        for col in df_def.columns[:-1]:\n","            if col.split('_')[0]=='12':\n","                df_def = df_def.loc[df_def[col]<1].reset_index(drop=True)\n","    return df_def"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nVyLh0T8IsQf"},"outputs":[],"source":["def kill_max_class(df_def):\n","    df_count = df_def.groupby('class', as_index=False).count()\n","    df_count.index = df_count['class']\n","    class_median = int(df_count[df_count.columns[1]].median())\n","    max_class = df_count[df_count.columns[1]].idxmax()\n","    df_def_max = df_def.loc[df_def['class']==max_class]\n","    df_def_max['ind'] = df_def_max.index\n","    df_def_max = df_def_max.loc[df_def_max['ind'].isin(random.sample(list(df_def_max.index), class_median))]\n","    df_def = pd.concat([df_def.loc[df_def['class']!=max_class], df_def_max[df_def_max.columns[:-1]]], ignore_index=True).sort_values(['class']).reset_index(drop=True)\n","    return df_def"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xrZK45aXjp0t"},"outputs":[],"source":["def write_geotiff(raster_input, raster_output, predictions):\n","    \"\"\"\n","    Function to replace values in geotiff upper then threshold by thresholf  🙂\n","    \n","    Input : GeoTIFF, threshold_value\n","    \n","    Output : GeoTIFF \n","    \n","    \"\"\"    \n","    in_data, out_data = None, None\n","\n","    # open input raster\n","    in_data = gdal.Open(raster_input)\n","    if in_data is None:\n","        print ('Unable to open %s' % raster_input)\n","        return None\n","\n","    # read in data from first band of input raster\n","    band1 = in_data.GetRasterBand(1)\n","    rows = in_data.RasterYSize\n","    cols = in_data.RasterXSize\n","    vals = band1.ReadAsArray(0, 0, cols, rows)\n","\n","    driver = in_data.GetDriver()\n","    out_data = driver.Create(raster_output, cols, rows, 1, GDT_Float32)\n","\n","    dem_data = np.array(predictions)\n","    out_band = out_data.GetRasterBand(1)\n","    out_band.WriteArray(dem_data)\n","    out_band.FlushCache()\n","    out_band.SetNoDataValue(-32767.)\n","\n","    out_data.SetGeoTransform(in_data.GetGeoTransform())\n","    out_data.SetProjection(in_data.GetProjection())\n","    del out_data\n","    return raster_output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HiPKoJc-YooG"},"outputs":[],"source":["path = \"Forest/data/\"\n","name_field = 'Bratsk'\n","resolution = '10'\n","folder_out = '10_new'\n","#folder_out = '10_only_forest_new'\n","\n","path_field = path + name_field + '/' + resolution + '/'\n","path_field_year = [path_field+f+'/' for f in os.listdir(path_field) if f!='Masks']\n","path_snapshots = [field_year+snapshot for field_year in path_field_year for snapshot in os.listdir(field_year)]\n","path_masks = path + name_field + '/' + 'Masks/In/' + os.listdir(path + name_field + '/' + 'Masks/In/')[0]\n","path_out_masks = path + name_field + '/' + 'Masks/Out/' + folder_out + '/'\n","\n","if not os.path.exists(path_out_masks):\n","    os.mkdir(path_out_masks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EVW13GGkxAZz"},"outputs":[],"source":["remove_autumn_months = True\n","if remove_autumn_months:\n","    while [path_snapshot for path_snapshot in path_snapshots if (int(path_snapshot.split('/')[-1].split('.')[0][6:7])<6 or int(path_snapshot.split('/')[-1].split('.')[0][6:7])>8)]:\n","        for path_snapshot in [path_snapshot for path_snapshot in path_snapshots if (int(path_snapshot.split('/')[-1].split('.')[0][6:7])<6 or int(path_snapshot.split('/')[-1].split('.')[0][6:7])>8)]:\n","            path_snapshots.remove(path_snapshot)"]},{"cell_type":"code","source":["count_cloud = float(\"inf\")\n","for path_snapshot in path_snapshots:\n","    if path_snapshot[-9]=='7' or path_snapshot[-9]=='8':\n","        #print(path_snapshot)\n","        field = np.array(gdal.Open(path_snapshot).ReadAsArray())\n","        df_i = pd.DataFrame(field.reshape(13, -1).transpose())\n","        if len(df_i.loc[df_i[12]>0])<count_cloud:\n","            count_cloud = len(df_i.loc[df_i[12]>0])\n","            path_min_cloud = path_snapshot\n","        if count_cloud==0:\n","            print(path_snapshot)\n","            #break"],"metadata":{"id":"X0QqdtVsQ3Fo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["field = np.array(gdal.Open(path_min_cloud).ReadAsArray())\n","get_img(field)\n","\n","with open(path_masks) as f:\n","    gj = geojson.load(f)\n","features = [feature[\"geometry\"] for feature in gj['features']]\n","\n","with rasterio.open(path_min_cloud) as src:\n","    out_image, _ = mask(src, features, crop=True)\n","\n","get_img(out_image)"],"metadata":{"id":"8vv1qGjtQzzy"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PnXUfMaxxnpw"},"outputs":[],"source":["def convert_to_geotiff(raster_input, raster_output, values_to_write):\n","    \"\"\"\n","    Save GPP resul geotiff GeoTiff\n","\n","    input: path to geotiff with fPAR \n","    output: path to new geotiff with GPP \n","    values_to_write: np.array with dimensions like input geotiff file\n","\n","    \"\"\"\n","    in_data, out_data = None, None\n","    in_data = gdal.Open(raster_input)\n","    if in_data is None:\n","        print ('Unable to open %s' % raster_input)\n","\n","    # read in data from first band of input raster\n","    band1 = in_data.GetRasterBand(1)\n","    rows = in_data.RasterYSize\n","    cols = in_data.RasterXSize\n","    vals = band1.ReadAsArray(0, 0, cols, rows)\n","\n","    driver = in_data.GetDriver()\n","\n","    num_of_layers = values_to_write.shape[0]\n","\n","    out_data = driver.Create(raster_output, cols, rows, num_of_layers, GDT_Float32)\n","\n","    # Save data\n","    for i in range(num_of_layers):\n","        out_band = out_data.GetRasterBand(i+1)\n","        out_band.WriteArray(values_to_write[i, :, :])\n","    out_band.FlushCache()\n","    out_band.SetNoDataValue(-32767.)\n","\n","    out_data.SetGeoTransform(in_data.GetGeoTransform())\n","    out_data.SetProjection(in_data.GetProjection())\n","    del out_data\n","    return raster_output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bnqtK8AucJSf"},"outputs":[],"source":["path_Haralick = '/Forest/HaralickTask_all_part_improve/'\n","path_Haralick_features = [path_Haralick+f+'/' for f in os.listdir(path_Haralick) if f!='tiff']\n","path_Haralick_npy = 'data/BANDS-S2-L2A_HARALICK.npy'"]},{"cell_type":"code","source":["path_terrain = '/Forest/Terrain/'"],"metadata":{"id":"JPwrL-ZqRDoV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-PZXyTOHckTo"},"outputs":[],"source":["input_raster = path_min_cloud\n","output_raster = path_Haralick+'tiff/'\n","if not os.path.exists(output_raster):\n","    os.mkdir(output_raster)\n","\n","field = np.array(gdal.Open(path_min_cloud).ReadAsArray())\n","Haralick_npy_shape = np.load(path_Haralick_features[0]+'0/'+path_Haralick_npy)[0]\n","for path_Haralick_feature in path_Haralick_features:\n","    if not os.path.exists(output_raster+path_Haralick_feature.split(path_Haralick)[1].split('/')[0]+'.tiff'):\n","        array_reloaded = np.zeros(shape=(field.shape[1], field.shape[2], Haralick_npy_shape.shape[2]))\n","        for i in range(4):\n","            array_reloaded[Haralick_npy_shape.shape[0]*(1-i%2):Haralick_npy_shape.shape[0]*(2-i%2),Haralick_npy_shape.shape[1]*(i//2):Haralick_npy_shape.shape[1]*(i//2+1),:] = np.load(path_Haralick_feature+str(i)+'/'+path_Haralick_npy)[0]\n","        res_fname = convert_to_geotiff(raster_input = input_raster, raster_output=output_raster+path_Haralick_feature.split(path_Haralick)[1].split('/')[0]+'.tiff', values_to_write=array_reloaded.transpose(2, 0, 1))\n","        print(res_fname)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dN82imY11cQX"},"outputs":[],"source":["path_Haralick_tiffs = [output_raster+f for f in os.listdir(output_raster)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JoV0-2RA9AQR"},"outputs":[],"source":["def generate_NDVI(features):\n","    nir = features[7]\n","    red = features[3]\n","    ndvi = (nir-red)/((nir+red).apply(lambda x: 0.000000001 if x==0 else x))\n","    return ndvi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IEyjYBbS9ATF"},"outputs":[],"source":["def generate_EVI(features):\n","    evi2 = 2.5*(features[7] - features[3])/((features[7]  + 2.4*features[3] + 1).apply(lambda x: 0.000000001 if x==0 else x))\n","    return evi2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"60POICW09hc-"},"outputs":[],"source":["def generate_NDRE(features):\n","    ndre = (features[7] - features[4])/((features[7] + features[4]).apply(lambda x: 0.000000001 if x==0 else x))\n","    return ndre"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0qE_jm5W9hfT"},"outputs":[],"source":["def generate_MSAVI(features):\n","    nir = features[7]\n","    red = features[3]\n","    msavi=(2*nir + 1 - ((2*nir+1)**2 - 8*(nir-red))**(1/2))/2\n","    return msavi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mNYKfj3-9kJQ"},"outputs":[],"source":["def generate_all_indices(df_def):\n","    df_def_col = list(df_def.columns)\n","    df_def['ndvi'] = generate_NDVI(df_def)\n","    df_def['evi'] = generate_EVI(df_def)\n","    df_def['ndre'] = generate_NDRE(df_def)\n","    df_def['msavi'] = generate_MSAVI(df_def)\n","    df_def = df_def.reindex(columns=df_def_col[:-2]+list(df_def.columns[-4:])+df_def_col[-2:])\n","    return df_def"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BwlB-7CtI5yq"},"outputs":[],"source":["def del_row(df_def, create_id_test=False):\n","    del_row_col = []\n","    for col in df_def.columns[:-1]:\n","        if not create_id_test:\n","            if (col[0]=='2' or col[0]=='3') and col[2]=='2':\n","                del_row_col.append(col)\n","        else:\n","            if str(col)[0]=='2' or str(col)[0]=='3':\n","                del_row_col.append(col)\n","    del_row_cond = (df_def[del_row_col[0]]>0)\n","    for col in del_row_col[1:]:\n","        del_row_cond = del_row_cond&(df_def[col]>0)\n","\n","    df_def = df_def.loc[del_row_cond].reset_index(drop=True)\n","    return df_def"]},{"cell_type":"code","source":["list_df = []\n","list_df_test = []\n","list_data = []\n","dict_field_id = {}\n","\n","part_test = 0.3\n","max_snapshot = 10000\n","max_shuffle = 30\n","flag_remove_cloud = False\n","create_only_field = False\n","kill_class = True\n","\n","max_class = float(\"inf\")\n","if create_only_field:\n","    max_class = 7\n","\n","with open(path_masks) as f:\n","    gj = geojson.load(f)\n","features_masks = [feature[\"geometry\"] for feature in gj['features']]\n","features_desc = [[feature[\"properties\"][\"t_Class\"], feature[\"properties\"][\"t_Клас\"]] for feature in gj['features']]\n","features_ind = [feature[\"properties\"][\"t_Class\"] for feature in gj['features']]\n","\n","dict_desc = {}\n","for key_i, i in list(set(tuple(row) for row in features_desc)):\n","    dict_desc[key_i] = i\n","\n","for snapshot_i, path_snapshot in enumerate(path_snapshots+path_Haralick_tiffs+path_terrain):\n","    if snapshot_i<max_snapshot:\n","        print(path_snapshot)\n","        field = np.array(gdal.Open(path_snapshot).ReadAsArray())\n","\n","        list_field = []\n","        for feature in features_masks:\n","            with rasterio.open(path_snapshot) as src:\n","                out_image, out_transform = mask(src, [feature], crop=True)\n","                list_field.append(out_image)\n","        \n","        list_field_ind_g = []\n","        features_ind_g = []\n","        for i in range(min(features_ind), max(features_ind)+1):\n","            indices_i = [feature_ind for feature_ind, x in enumerate(features_ind) if x == i]\n","            if len(indices_i)>0:\n","                list_field_ind_g.append([list_field[index] for index in indices_i])\n","                features_ind_g.append(i)\n","        \n","        max_canal = list_field[0].shape[0]\n","        \n","        num_field = 0\n","        df = pd.DataFrame(columns=[i for i in range(max_canal)]+['class'])\n","        df_test = pd.DataFrame(columns=[i for i in range(max_canal)]+['class'])\n","\n","        for ind_g, field_g_i in zip(features_ind_g, list_field_ind_g):\n","            df_g_i, num_field = create_df(field_g_i, ind_g, max_canal, num_field)\n","            if snapshot_i==0:\n","                df_g_i_0 = df_g_i.copy()\n","                df_g_i_0 = del_row(df_g_i_0, create_id_test=True)\n","                id_field = get_id_test(df_g_i_0, part_test, max_shuffle)\n","                dict_field_id[ind_g] = id_field\n","            else:\n","                id_field = dict_field_id[ind_g]\n","            if id_field:\n","                df = pd.concat([df, df_g_i.loc[~df_g_i['id'].isin(id_field)]], ignore_index=True)\n","                df_test = pd.concat([df_test, df_g_i.loc[df_g_i['id'].isin(id_field)]], ignore_index=True)\n","            else:\n","                for num_field_i in df_g_i['id'].unique():\n","                    df_i = df_g_i.loc[df_g_i['id']==num_field_i].reset_index(drop=True)\n","                    test_ind = int(round(len(df_i)*part_test))\n","                    df = pd.concat([df, df_i.loc[df_i.index[:-test_ind]]], ignore_index=True)\n","                    df_test = pd.concat([df_test, df_i.loc[df_i.index[-test_ind:]]], ignore_index=True)\n","        \n","        if snapshot_i<len(path_snapshots):\n","            df = generate_all_indices(df.copy())\n","            df_test = generate_all_indices(df_test.copy())\n","        \n","        list_df.append(df)\n","        list_df_test.append(df_test)\n","        list_data.append(path_snapshot.split('/')[-1].split('.')[0])\n","\n","for list_i in [list_df, list_df_test]:\n","    dev_df = []\n","    for df_i in list_i:\n","        dev_df.append(len(df_i))\n","    if len(set(dev_df))>1:\n","        print('Разная длина у DataFrame')\n","\n","df = create_prepared_df(list_df, list_data, remove_cloud=flag_remove_cloud, with_class=True)\n","df_test = create_prepared_df(list_df_test, list_data, remove_cloud=flag_remove_cloud, with_class=True)\n","\n","df = del_row(df)\n","df_test = del_row(df_test)\n","\n","if kill_class:\n","    df = kill_max_class(df)\n","    df_test = kill_max_class(df_test)\n","\n","for col in df.columns:\n","    df[col] = pd.to_numeric(df[col])\n","for col in df_test.columns:\n","    df_test[col] = pd.to_numeric(df_test[col])\n","\n","if create_only_field:\n","    df = df.loc[df['class']<=max_class].reset_index(drop=True)\n","    df_test = df_test.loc[df_test['class']<=max_class].reset_index(drop=True)"],"metadata":{"id":"tArQ9BEmQv50"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jpV_izMgsKUy"},"outputs":[],"source":["# norm image\n","norm_image = False\n","if norm_image:\n","    for col in df.columns:\n","        if col.split('_')[0] not in ['ndvi', 'evi', 'ndre', 'msavi', 'class', '12']:\n","            df[col] = (df[col]-df[col].mean())/df[col].std()\n","            df_test[col] = (df_test[col]-df_test[col].mean())/df_test[col].std()"]},{"cell_type":"code","source":["for col in df.columns[:-1]:\n","    if col.split('_')[0] in ['1','2','3','7','8','12']:\n","      del df[col]\n","for col in df_test.columns[:-1]:\n","    if col.split('_')[0] in ['1','2','3','7','8','12']:\n","      del df_test[col]"],"metadata":{"id":"E-TcxM-nmM9E"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MYgJwwFR9rCd"},"outputs":[],"source":["X_train = df[df.columns[:-1]]\n","y_train = list(df['class'])\n","X_test = df_test[df_test.columns[:-1]]\n","y_test = list(df_test['class'])"]},{"cell_type":"code","source":["\n","base_classfiers = [KNeighborsClassifier(n_jobs=-1, algorithm='ball_tree', leaf_size=100, n_neighbors=10, weights='uniform'),\n","                   DecisionTreeClassifier(random_state=42, criterion='entropy', max_depth=9, max_features=None, min_samples_leaf=3, min_samples_split=2, splitter='best'),\n","                   RandomForestClassifier(n_jobs=-1, random_state=42, criterion='gini', max_features='auto', class_weight=None, max_depth=50, n_estimators=500, min_samples_leaf=2, min_samples_split=6),\n","                   ExtraTreesClassifier(n_jobs=-1, random_state=42, criterion='entropy', max_depth=9, max_features='log2', min_samples_leaf=5, min_samples_split=2, n_estimators=150),\n","                   RidgeClassifier(random_state=42, solver='sag', class_weight=None, fit_intercept=True, normalize=True, alpha=1.1, tol=1e-5),\n","                   LogisticRegression(n_jobs=-1, random_state=42, class_weight='balanced', dual=False, fit_intercept=False, C=1.2, max_iter=100, tol=1e-04, penalty='l1', solver='saga'),\n","                   SVC(random_state=42, gamma='scale', kernel='poly', C=1, degree=1, tol=1e-5, probability=True),\n","                   XGBClassifier(n_jobs=-1, tree_method='gpu_hist', predictor='gpu_predictor', booster='gblinear', eta=0.3, gamma='auto', max_depth=20)\n","                   ]\n","name_classfiers = ['KNeighborsClassifier',\n","                   'DecisionTreeClassifier',\n","                   'RandomForest',\n","                   'ExtraTreesClassifier',\n","                   'RidgeClassifier',\n","                   'LogisticRegression',\n","                   'SVC',\n","                   'XGBClassifier'\n","                   ]\n","\n","for i in range(len(base_classfiers)):\n","    base_classfiers[i].fit(X_train, y_train)\n","    print('Done: '+name_classfiers[i])"],"metadata":{"id":"VofYgiLlRV7i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["score_classfiers_accuracy_score = []\n","score_classfiers_roc_auc_score = []\n","score_classfiers_f1_score = []\n","df_score_class_list = []\n","for i in range(len(base_classfiers)):\n","    y_predict = base_classfiers[i].predict(X_test)\n","    score_classfiers_accuracy_score.append(accuracy_score(y_test, y_predict))\n","    if name_classfiers[i]!='RidgeClassifier':\n","        score_classfiers_roc_auc_score.append(roc_auc_score(y_test, base_classfiers[i].predict_proba(X_test), multi_class='ovr'))\n","    else:\n","        ridge_predict = []\n","        for k in range(len(X_test)):\n","            d = base_classfiers[i].decision_function(X_test)[k]\n","            probs = np.exp(d) / np.sum(np.exp(d))\n","            ridge_predict.append(probs)\n","        ridge_predict = np.array(ridge_predict)\n","        score_classfiers_roc_auc_score.append(roc_auc_score(y_test, ridge_predict, multi_class='ovr')) \n","\n","    score_classfiers_f1_score.append(f1_score(y_test, y_predict, average='weighted'))\n","    df_i = pd.DataFrame(metrics.classification_report(y_test, y_predict, digits=2, output_dict=True)).transpose()\n","    arrays_col = [\n","        [name_classfiers[i], name_classfiers[i], name_classfiers[i], name_classfiers[i]],\n","        list(df_i.columns),\n","    ]\n","    df_i.columns = pd.MultiIndex.from_tuples(list(zip(*arrays_col)))\n","    df_score_class_list.append(df_i)\n","\n","df_score_class = df_score_class_list[0]\n","for i in range(1,len(df_score_class_list)):\n","    df_score_class = df_score_class.join(df_score_class_list[i])\n","df_score_class_index = list(df_score_class.index)\n","for i, ind in enumerate(df_score_class_index):\n","    if ind.isdigit():\n","        df_score_class_index[i] = dict_desc[int(ind)]\n","df_score_class.index = df_score_class_index\n","\n","df_score_group = pd.DataFrame(columns=['Model', 'Accuracy score', 'ROC AUC score', 'f1 score'])\n","df_score_group['Model'] = name_classfiers\n","df_score_group['Accuracy score'] = score_classfiers_accuracy_score\n","df_score_group['ROC AUC score'] = score_classfiers_roc_auc_score\n","df_score_group['f1 score'] = score_classfiers_f1_score\n","\n","#df_score_group.to_excel(path_out_masks+'score_by_model_roc.xlsx', index=False)\n","#df_score_class.to_excel(path_out_masks+'score_by_group_roc.xlsx')"],"metadata":{"id":"I9KWWCncRdPy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["list_df_fi = []\n","list_data_fi = []\n","for snapshot_i, path_snapshot in enumerate(path_snapshots+path_Haralick_tiffs):\n","    if snapshot_i<max_snapshot:\n","        print(path_snapshot)\n","        field = np.array(gdal.Open(path_snapshot).ReadAsArray())\n","        max_canal = field.shape[0]\n","        df_i = pd.DataFrame(field.reshape(max_canal, -1).transpose())\n","        list_df_fi.append(df_i)\n","        list_data_fi.append(path_snapshot.split('/')[-1].split('.')[0])\n","        if snapshot_i == 0:\n","            field_shape = field.shape\n","\n","dev_df = []\n","for df_i in list_df_fi:\n","    dev_df.append(len(df_i))\n","if len(set(dev_df))>1:\n","    print('Разная длина у DataFrame')\n","\n","#df_res = create_prepared_df(list_df_fi, list_data_fi, remove_cloud=False, with_class=False)\n","\n","#del list_df_fi\n","#del df_i\n","\n","#list_df_res = []\n","#for i in range(int(len(df_res)/5000)+1):\n","#    list_df_res.append(df_res[i*5000:(i+1)*5000])\n","\n","list_df_res = []\n","len_df_fi = len(list_df_fi[0])\n","for i in range(int(len_df_fi/500000)+1):\n","    list_df_res.append(pd.DataFrame())\n","for data_i, df_i in zip(list_data_fi, list_df_fi):\n","    df_i = df_i.rename(columns={x:str(x)+'_'+data_i for x in df_i.columns})\n","    for i in range(int(len_df_fi/500000)+1):\n","        list_df_res[i] = list_df_res[i].join(df_i[i*500000:(i+1)*500000], how=\"outer\")\n","\n","del list_df_fi\n","del df_i\n","\n","for i in range(len(base_classfiers)):\n","    res = np.empty([0], dtype=int)\n","    for df_res_i in list_df_res:\n","        res = np.concatenate((res, base_classfiers[i].predict(df_res_i)))\n","    print('Подготовка к сохранению')\n","    outPut = write_geotiff(path_min_cloud, path_out_masks+name_classfiers[i]+'_predictions_har.tif', res.reshape(field_shape[1:]))\n","    print('Done: '+name_classfiers[i])"],"metadata":{"id":"F49TeRfnRs9z"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6caT-GqTRbYE"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}